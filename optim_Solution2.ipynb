{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cb82b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T09:09:58.259132Z",
     "start_time": "2025-10-19T09:08:21.033672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in ./.venv/lib/python3.9/site-packages (1.78.5)\n",
      "Requirement already satisfied: langchain_ollama in ./.venv/lib/python3.9/site-packages (0.3.10)\n",
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.9/site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain_anthropic in ./.venv/lib/python3.9/site-packages (0.3.22)\n",
      "Requirement already satisfied: langchain-tavily in ./.venv/lib/python3.9/site-packages (0.2.11)\n",
      "Requirement already satisfied: langchain_experimental in ./.venv/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.9/site-packages (0.6.10)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./.venv/lib/python3.9/site-packages (from litellm) (3.13.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from litellm) (8.1.8)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in ./.venv/lib/python3.9/site-packages (from litellm) (0.13.5)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./.venv/lib/python3.9/site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./.venv/lib/python3.9/site-packages (from litellm) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in ./.venv/lib/python3.9/site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./.venv/lib/python3.9/site-packages (from litellm) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in ./.venv/lib/python3.9/site-packages (from litellm) (2.5.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in ./.venv/lib/python3.9/site-packages (from litellm) (2.12.3)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in ./.venv/lib/python3.9/site-packages (from litellm) (1.1.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./.venv/lib/python3.9/site-packages (from litellm) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in ./.venv/lib/python3.9/site-packages (from litellm) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.9/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.9/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.9/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.9/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in ./.venv/lib/python3.9/site-packages (from langchain_ollama) (0.6.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.76 in ./.venv/lib/python3.9/site-packages (from langchain_ollama) (0.3.79)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./.venv/lib/python3.9/site-packages (from langchain-core<2.0.0,>=0.3.76->langchain_ollama) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<2.0.0,>=0.3.76->langchain_ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<2.0.0,>=0.3.76->langchain_ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<2.0.0,>=0.3.76->langchain_ollama) (6.0.3)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<2.0.0,>=0.3.76->langchain_ollama) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx>=0.23.0->litellm) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx>=0.23.0->litellm) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx>=0.23.0->litellm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in ./.venv/lib/python3.9/site-packages (from langchain_community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in ./.venv/lib/python3.9/site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in ./.venv/lib/python3.9/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in ./.venv/lib/python3.9/site-packages (from langchain_community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.9/site-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.9/site-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.9/site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.76->langchain_ollama) (2.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: anthropic<1.0.0,>=0.69.0 in ./.venv/lib/python3.9/site-packages (from langchain_anthropic) (0.71.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from anthropic<1.0.0,>=0.69.0->langchain_anthropic) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in ./.venv/lib/python3.9/site-packages (from anthropic<1.0.0,>=0.69.0->langchain_anthropic) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from anthropic<1.0.0,>=0.69.0->langchain_anthropic) (0.11.1)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from anthropic<1.0.0,>=0.69.0->langchain_anthropic) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx>=0.23.0->litellm) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in ./.venv/lib/python3.9/site-packages (from langgraph) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in ./.venv/lib/python3.9/site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in ./.venv/lib/python3.9/site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in ./.venv/lib/python3.9/site-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in ./.venv/lib/python3.9/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.11.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.9/site-packages (from openai>=1.99.5->litellm) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.9/site-packages (from tiktoken>=0.7.0->litellm) (2025.9.18)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./.venv/lib/python3.9/site-packages (from tokenizers->litellm) (0.35.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.1.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U litellm langchain_ollama langchain_community langchain_anthropic langchain-tavily langchain_experimental matplotlib langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83981318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.9/site-packages (0.3.35)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.78 in ./.venv/lib/python3.9/site-packages (from langchain_openai) (0.3.79)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in ./.venv/lib/python3.9/site-packages (from langchain_openai) (2.5.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from langchain_openai) (0.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.12.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./.venv/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.9/site-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.9/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.9.18)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3108cb5725fc6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:16.411142Z",
     "start_time": "2025-10-19T15:45:52.620481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaroslav/final-sql/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c529c44a82b3ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:17.489632Z",
     "start_time": "2025-10-19T15:46:16.419124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_gen_qwen3_coder инициализирован для модели: qwen3-coder:30b\n"
     ]
    }
   ],
   "source": [
    "llm_gen_qwen3_coder = ChatOpenAI(\n",
    "    model=\"qwen3-coder:30b\",  # Указанная модель\n",
    "    openai_api_base=\"https://cloud.m1r0.ru/v1\",\n",
    "    openai_api_key=\"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\",\n",
    "    temperature=0\n",
    ")\n",
    "print(f\"llm_gen_qwen3_coder инициализирован для модели: {llm_gen_qwen3_coder.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f492bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:21.474098Z",
     "start_time": "2025-10-19T15:46:21.467461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['url', 'ddl', 'queries']) dict_keys(['queryid', 'query', 'runquantity', 'executiontime'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"trash/questsH.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data.keys(), data['queries'][0].keys())\n",
    "\n",
    "DDL_querys = \"\\n\".join([i['statement'] for i in data['ddl']])\n",
    "SQL_querys = [i['query'] for i in data['queries']]\n",
    "\n",
    "defolt_retry_nums = 3\n",
    "MAX_CONCURRENT_CALLS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5569abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: quests public -> TARGET: public_v2\n"
     ]
    }
   ],
   "source": [
    "# [CELL 1] ВСТАВИТЬ СРАЗУ ПОСЛЕ ЗАГРУЗКИ ВХОДА В data\n",
    "# - Определяет текущие <CATALOG>.<SOURCE_SCHEMA> голосованием по полным именам\n",
    "# - Устанавливает CATALOG, SOURCE_SCHEMA и TARGET_SCHEMA (при необходимости переименуйте)\n",
    "\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Iterable, Tuple\n",
    "\n",
    "_FQ_TBL = r\"[A-Za-z_]\\w*\\.[A-Za-z_]\\w*\\.[A-Za-z_]\\w*\"\n",
    "\n",
    "def _iter_fq_pairs_from_sql(sql: str) -> Iterable[str]:\n",
    "    if not sql:\n",
    "        return ()\n",
    "    # FROM / JOIN источники\n",
    "    for pat in (rf\"(?i)\\bFROM\\s+({_FQ_TBL})\", rf\"(?i)\\bJOIN\\s+({_FQ_TBL})\"):\n",
    "        for m in re.finditer(pat, sql):\n",
    "            yield \".\".join(m.group(1).split(\".\")[:2])\n",
    "    # Явные цели: CREATE/ALTER/INSERT INTO <catalog.schema.table>\n",
    "    for m in re.finditer(rf\"(?i)\\b(CREATE|ALTER|INSERT\\s+INTO)\\s+(?:TABLE\\s+)?({_FQ_TBL})\", sql):\n",
    "        yield \".\".join(m.group(2).split(\".\")[:2])\n",
    "\n",
    "def collect_catalog_schema_pairs(data: Dict) -> Counter:\n",
    "    pairs: Counter = Counter()\n",
    "    for d in (data.get(\"ddl\") or []):\n",
    "        stmt = (d or {}).get(\"statement\") or \"\"\n",
    "        for pair in _iter_fq_pairs_from_sql(stmt):\n",
    "            pairs[pair] += 1\n",
    "    for q in (data.get(\"queries\") or []):\n",
    "        sql = (q or {}).get(\"query\") or \"\"\n",
    "        for pair in _iter_fq_pairs_from_sql(sql):\n",
    "            pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "def decide_by_vote(pairs: Counter, majority: float = 0.8) -> Tuple[str, str]:\n",
    "    if not pairs:\n",
    "        raise ValueError(\"No fully-qualified table names found to determine <catalog>.<schema>\")\n",
    "    most, cnt = pairs.most_common(1)[0]\n",
    "    total = sum(pairs.values())\n",
    "    if cnt == total or (total and cnt / total >= majority):\n",
    "        cat, sch = most.split(\".\")\n",
    "        return cat, sch\n",
    "    raise ValueError(f\"Ambiguous schemas: {dict(pairs)} (no majority >= {majority})\")\n",
    "\n",
    "def detect_catalog_schema(data: Dict, majority: float = 0.8, as_json: bool = False):\n",
    "    pairs = collect_catalog_schema_pairs(data)\n",
    "    catalog, schema = decide_by_vote(pairs, majority)\n",
    "    return {\"catalog\": catalog, \"schema\": schema} if as_json else f\"{catalog}.{schema}\"\n",
    "\n",
    "# Определяем текущие <catalog>.<schema> и задаём целевую схему\n",
    "CATALOG, SOURCE_SCHEMA = detect_catalog_schema(data, as_json=False).split('.')\n",
    "TARGET_SCHEMA = f\"{SOURCE_SCHEMA}_v2\"  # при необходимости задайте явно, например: 'quests_v2'\n",
    "\n",
    "print(\"Detected:\", CATALOG, SOURCE_SCHEMA, \"-> TARGET:\", TARGET_SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4531277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Утилиты нормализации SQL для сборки результата\n",
    "import re\n",
    "\n",
    "def split_sql(sql: str):\n",
    "    out, current, in_quote = [], [], False\n",
    "    for ch in sql or \"\":\n",
    "        if ch == \"'\" and (not current or current[-1] != '\\\\'):\n",
    "            in_quote = not in_quote\n",
    "        if ch == ';' and not in_quote:\n",
    "            stmt = ''.join(current).strip()\n",
    "            if stmt:\n",
    "                out.append(stmt)\n",
    "            current = []\n",
    "        else:\n",
    "            current.append(ch)\n",
    "    tail = ''.join(current).strip()\n",
    "    if tail:\n",
    "        out.append(tail)\n",
    "    return out\n",
    "\n",
    "def _strip_name(name: str) -> str:\n",
    "    return name.strip().strip('`').strip('\\\"')\n",
    "\n",
    "def ensure_target(name: str) -> str:\n",
    "    parts = [p for p in _strip_name(name).split('.') if p]\n",
    "    tbl = parts[-1] if parts else _strip_name(name)\n",
    "    return f\"{CATALOG}.{TARGET_SCHEMA}.{tbl}\"\n",
    "\n",
    "def ensure_source(name: str) -> str:\n",
    "    parts = [p for p in _strip_name(name).split('.') if p]\n",
    "    tbl = parts[-1] if parts else _strip_name(name)\n",
    "\n",
    "    return f\"{CATALOG}.{SOURCE_SCHEMA}.{tbl}\"\n",
    "\n",
    "def normalize_ctas_order(stmt: str) -> str:\n",
    "    pattern = re.compile(r'(?i)(CREATE\\s+TABLE\\s+[^\\s]+)\\s+AS\\s+(SELECT\\b.*)\\s+WITH\\s*\\(([^)]*)\\)')\n",
    "\n",
    "    def repl(match: re.Match) -> str:\n",
    "        create_part, select_part, with_part = match.group(1), match.group(2), match.group(3)\n",
    "        return f\"{create_part} WITH ({with_part}) AS {select_part}\"\n",
    "\n",
    "    return re.sub(pattern, repl, stmt)\n",
    "\n",
    "def rewrite_targets_sources(stmt: str) -> str:\n",
    "    s = stmt\n",
    "    s = re.sub(r'(?i)\\bCREATE\\s+TABLE\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"CREATE TABLE {ensure_target(m.group(1))}\", s)\n",
    "    s = re.sub(r'(?i)\\bINSERT\\s+INTO\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"INSERT INTO {ensure_target(m.group(1))}\", s)\n",
    "    s = re.sub(r'(?i)\\bMERGE\\s+INTO\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"MERGE INTO {ensure_target(m.group(1))}\", s)\n",
    "    s = re.sub(r'(?i)\\bUPDATE\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"UPDATE {ensure_target(m.group(1))}\", s)\n",
    "    s = re.sub(r'(?i)\\bFROM\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"FROM {ensure_source(m.group(1))}\", s)\n",
    "    s = re.sub(r'(?i)\\bJOIN\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"JOIN {ensure_source(m.group(1))}\", s)\n",
    "    s = re.sub(r'(?i)\\bUSING\\s+([A-Za-z0-9_\\.]+)', lambda m: f\"USING {ensure_source(m.group(1))}\", s)\n",
    "    s = normalize_ctas_order(s)\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def qualify_query_to_target(sql: str) -> str:\n",
    "    s = (sql or '').replace(f\"{CATALOG}.{SOURCE_SCHEMA}.\", f\"{CATALOG}.{TARGET_SCHEMA}.\")\n",
    "    return ' '.join(s.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0594c4fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:23.998553Z",
     "start_time": "2025-10-19T15:46:23.994004Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_analize_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Упрощённый промпт для query_analize_agent.\n",
    "    Агент анализирует до 5 SQL-запросов и возвращает текстовое описание закономерностей,\n",
    "    часто используемых таблиц, соединений, фильтров и потенциальных точек оптимизации под DDL.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — SQL-аналитик для системы оптимизации Data Lakehouse (Trino + Iceberg + S3).\n",
    "Вход: до 5 SQL-запросов.  \n",
    "Твоя цель — описать словами структуру и частые паттерны этих запросов, указав:\n",
    "- какие таблицы чаще всего участвуют;\n",
    "- какие поля часто используются в фильтрах (WHERE/ON);\n",
    "- по каким полям часто происходит соединение (JOIN);\n",
    "- какие операции чаще всего выполняются (GROUP BY, ORDER BY, DISTINCT, WINDOW);\n",
    "- какие конструкции создают нагрузку или могут быть оптимизированы через DDL;\n",
    "- какие улучшения можно предложить на уровне DDL (партиционирование, денормализация, сортировка и т. п.).\n",
    "\n",
    "### Важно:\n",
    "0. Обращай внимание на частоту выполнения запроса и его время, чтобы в первую очередь оптимизировать тяжёлые запросы\n",
    "1. **Запрещено** предлагать или использовать materialized views.\n",
    "2. **Запрещено** менять DDL или писать SQL-код миграций. Только описывать, что требует оптимизации.  \n",
    "3. **Запрещено** придумывать статистику, размеры таблиц или время выполнения. Если данных нет — укажи \"неизвестно\".  \n",
    "4. **Не упоминай** индексы в классическом RDBMS-смысле (B-Tree и т. д.).  \n",
    "5. **Не упоминай** безопасность, авторизацию, шифрование и внешние системы.  \n",
    "6. Используй формулировки, применимые к Trino + Iceberg + S3.  \n",
    "   (Например: «можно рассмотреть партиционирование по дате» вместо «создать индекс».)  \n",
    "7. Не возвращай JSON, таблицы или списки ключей. Пиши связный текстовый отчёт с пунктами и примерами.\n",
    "\n",
    "### Структура ответа:\n",
    "- Краткое резюме: какие таблицы и поля чаще всего встречаются.  \n",
    "- Анализ соединений: какие таблицы чаще соединяются между собой, по каким ключам.  \n",
    "- Анализ фильтров: какие колонки часто участвуют в WHERE/ON (например, event_date, user_id).  \n",
    "- Анализ операций: какие операции создают нагрузку (JOIN, GROUP BY, DISTINCT, ORDER BY, WINDOW).\n",
    "\n",
    "Вывод должен быть понятным, логически структурированным и готовым для последующей обработки другой моделью, которая будет строить DDL и миграции на основе твоего анализа.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6cb0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.165129Z",
     "start_time": "2025-10-19T15:46:24.159866Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_summarize_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Возвращает промпт (на русском) для query_summarize_agent.\n",
    "    Агент получает агрегированные результаты от query_analize_agent (строка/батч)\n",
    "    и должен выдать компактный, читаемый человеком и машиной, сводный отчёт\n",
    "    — пригодный для подачи в DDL_agent и migrations_agent.\n",
    "    \"\"\"\n",
    "\n",
    "    return \"\"\"Ты — агрегирующий аналитик SQL для пайплайна оптимизации Data Lakehouse (стек: Trino + Iceberg + S3).\n",
    "\n",
    "ВХОД:\n",
    "- Строка / текст `analize` — объединённые результаты работы query_analize_agent для многих батчей (каждый блок содержит анализ до 5 SQL-запросов).\n",
    "- Формат входа может быть JSON-подобным или текстовым отчётом (см. предыдущий агент). Разбирай оба варианта; если не можешь распарсить фрагмент — помечь как \"UNPARSED fragment\".\n",
    "\n",
    "ЦЕЛЬ:\n",
    "- Объединить и агрегировать мелкие анализы в единый сводный отчёт, выявив:\n",
    "  1. \"Hot\" таблицы — таблицы с наибольшей частотой упоминаний в тяжёлых операциях;\n",
    "  2. Частые пары JOIN (и их join_keys);\n",
    "  3. Часто используемые фильтры (колонки в WHERE/ON);\n",
    "  4. Частые тяжёлые операции (GROUP BY, ORDER BY, DISTINCT, WINDOW, SORT);\n",
    "  5. Повторяющиеся паттерны, которые можно исправить DDL-изменениями;\n",
    "  6. Очередность (приоритет) изменений для downstream: DDL_agent -> migrations_agent -> query_optimizer.\n",
    "- Сформировать короткий и однозначный набор рекомендаций уровня DDL (НЕ писать сам DDL/миграции, а только что и где изменить и почему).\n",
    "\n",
    "СТРОГИЕ ЗАПРЕЩЕНИЯ (выполняй обязательно):\n",
    "- НЕЛЬЗЯ предлагать или использовать materialized views.\n",
    "- НЕЛЬЗЯ писать, изменять или генерировать DDL или миграции — это делают downstream агенты.\n",
    "- НЕЛЬЗЯ придумывать статистику, объёмы данных, кардинальности или время выполнения; если этих метрик нет — указывай `UNKNOWN`.\n",
    "- НЕЛЬЗЯ предлагать классические RDBMS-индексы (B-Tree и т.п.). Если предлагаешь что-то похожее, опиши его как \"файловая/партиц./сортировка/кластеризация в Iceberg\" и пометь `depends_on_iceberg_features`.\n",
    "- НЕЛЬЗЯ обсуждать авторизацию/аутентификацию/безопасность/внешние сервисы.\n",
    "- Ответ должен быть ТОЛЬКО текстом отчёта (без генерации JSON/DDL/SQL). Пиши структурированный человекочитаемый отчёт (см. формат ниже).\n",
    "\n",
    "ПРАВИЛА АГРЕГАЦИИ:\n",
    "- Подсчитывай частоту встречаемости сущностей (таблиц, колонок, join-пар) по данным `analize`. Если входы не содержат явного счётчика — используй относительную частоту (high/medium/low) основанную на количестве вхождений в анализах; если нельзя определить — ставь `UNKNOWN`.\n",
    "- Если множество анализов указывает на фильтрацию по колонке `event_date`/`ds` — предлагай PARTITIONING (указывай день/месяц как опции). Добавляй замечание о риске при высокой кардинальности.\n",
    "- Если одна и та же пара таблиц соединяется очень часто — размышляй о DENORMALIZATION (опиши, какие поля включить), но НЕ генерируй CREATE TABLE.\n",
    "- Для ORDER BY + LIMIT — указывай возможность SORT_ORDER / предварительной кластеризации в Iceberg и помечай `depends_on_iceberg_features`.\n",
    "- В каждой рекомендации указывай: цель (что менять), конкретные колонки/таблицы, краткая причина (основанную на evidence fragments из `analize`), ориентировочный impact (HIGH/MEDIUM/LOW/UNKNOWN) и какие метрики/проверки нужны (data_volume, cardinality, query_frequency, avg_runtime).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f88a366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.350149Z",
     "start_time": "2025-10-19T15:46:24.342409Z"
    }
   },
   "outputs": [],
   "source": [
    "def ddl_optim_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Промпт для агента ddl_optimize_agent.\n",
    "    Цель — на основе анализа (query_summarize) и оригинальных DDL (ddl_orig)\n",
    "    выдать только новые или модифицированные DDL-запросы, оптимизирующие работу под Trino + Iceberg + S3.\n",
    "    Без пояснений, текста и комментариев.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Ты — DDL-оптимизатор для Trino + Iceberg + S3.\n",
    "\n",
    "[Контекст выполнения]\n",
    "- Catalog: {CATALOG}\n",
    "- Source schema (исходные таблицы): {SOURCE_SCHEMA}\n",
    "- Target schema (новая структура): {TARGET_SCHEMA}\n",
    "\n",
    "[Обязательные правила]\n",
    "- Полная квалификация всех таблиц: {CATALOG}.<schema>.<table>\n",
    "- Исходные таблицы в {CATALOG}.{SOURCE_SCHEMA}.* НЕ изменять и не использовать для записи:\n",
    "  запрещены ALTER/DROP/INSERT/UPDATE/MERGE/DELETE/CREATE VIEW/MATERIALIZED VIEW/GRANT/REVOKE для этой схемы.\n",
    "- Все новые объекты создавать и изменять ТОЛЬКО в {CATALOG}.{TARGET_SCHEMA}.*.\n",
    "- Первая DDL-команда должна быть: CREATE SCHEMA {CATALOG}.{TARGET_SCHEMA}.\n",
    "- Для CTAS соблюдай синтаксис Trino: пишем `CREATE TABLE <...> WITH (...) AS SELECT ...` (WITH перед AS) и не помещаем текст SELECT внутрь partitioning/ARRAY. {CATALOG}.{TARGET_SCHEMA}.\n",
    "- Не использовать materialized view и несоответствующие Trino/Iceberg свойства.\n",
    "\n",
    "Вход:\n",
    "- query_summarize: анализ частых фильтров, сортировок, JOIN\n",
    "- ddl_orig: исходные DDL таблиц\n",
    "\n",
    "Задача: Создать оптимизированные DDL для Trino + Iceberg.\n",
    "\n",
    "СИНТАКСИС Trino + Iceberg:\n",
    "- Таблицы: <catalog>.<schema>.<table>\n",
    "- Формат хранения файлов: используйте WITH (format = 'PARQUET')\n",
    "- Партиционирование (Trino/Iceberg): в CREATE TABLE через WITH (partitioning = ARRAY['day(ts_col)'] или 'month(ts_col)']). Не комбинировать year/month/day на одном столбце.\n",
    "- Свойства: только допустимые Trino/Iceberg ключи в WITH (...). Не использовать 'write.target-file-size-bytes'. Без висячих запятых.\n",
    "\n",
    "РАЗРЕШЕНО:\n",
    "- PARTITIONING (через WITH) с допустимыми функциями\n",
    "- CREATE TABLE AS SELECT WITH (format = 'PARQUET')\n",
    "- Создание оптимизированных копий\n",
    "\n",
    "ЗАПРЕЩЕНО:\n",
    "- Materialized Views, индексы\n",
    "- DROP, DELETE, RENAME\n",
    "- Несовместимый синтаксис\n",
    "\n",
    "ПРИМЕРЫ:\n",
    "ALTER TABLE analytics.sales.orders PARTITIONING (через WITH) year(order_date);\n",
    "CREATE TABLE analytics.sales.orders_new AS SELECT * FROM orders WITH (format = 'PARQUET');\n",
    "ALTER TABLE analytics.sales.orders -- removed unsafe SET PROPERTIES example\n",
    "\n",
    "ПРАВИЛА ВЫВОДА:\n",
    "- Каждый оператор — отдельной строкой; не объединяй множество операторов в одну строку.\n",
    "- Полные имена таблиц (<catalog>.<schema>.<table>) согласованы с JDBC url.\n",
    "- Никаких комментариев, префиксов типа \"sql \". Только чистые SQL-операторы.\n",
    "\n",
    "ВЫВОД: Только SQL DDL команды, готовые к выполнению в Trino.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9534a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.501385Z",
     "start_time": "2025-10-19T15:46:24.497467Z"
    }
   },
   "outputs": [],
   "source": [
    "def migrations_creator_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Промпт для агента migrations_creator_agent.\n",
    "    Агент получает:\n",
    "      - query_summarize — сводный анализ SQL-запросов,\n",
    "      - new_ddl — новые DDL-запросы, сгенерированные ddl_optim_agent.\n",
    "    Его задача — сгенерировать SQL-миграции для применения новых DDL в безопасном формате,\n",
    "    совместимом с Trino + Iceberg + S3.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Ты — генератор миграций для Trino + Iceberg + S3.\n",
    "\n",
    "[Контекст выполнения]\n",
    "- Catalog: {CATALOG}\n",
    "- Source schema (исходные таблицы): {SOURCE_SCHEMA}\n",
    "- Target schema (новая структура): {TARGET_SCHEMA}\n",
    "\n",
    "[Обязательные правила]\n",
    "- Полная квалификация всех таблиц: {CATALOG}.<schema>.<table>\n",
    "- Исходные таблицы в {CATALOG}.{SOURCE_SCHEMA}.* НЕ изменять и не использовать для записи:\n",
    "  запрещены ALTER/DROP/INSERT/UPDATE/MERGE/DELETE/CREATE VIEW/MATERIALIZED VIEW/GRANT/REVOKE для этой схемы.\n",
    "- Миграции пишут данные ТОЛЬКО в {CATALOG}.{TARGET_SCHEMA}.*, читают ТОЛЬКО из {CATALOG}.{SOURCE_SCHEMA}.*\n",
    "- Не дублировать DDL из new_ddl: в миграциях не должно быть CREATE TABLE/CTAS уже присутствующих в new_ddl.\n",
    "- Не использовать materialized view, несоответствующие Trino/Iceberg свойства, авторизацию/GRANT/REVOKE.\n",
    "- Возвращай только SQL-команды миграций. Без пояснений, без комментариев. Каждая команда на новой строке.\n",
    "\n",
    "ВХОД:\n",
    "- query_summarize: анализ использования таблиц\n",
    "- new_ddl: оптимизированные DDL-запросы\n",
    "\n",
    "ЦЕЛЬ:\n",
    "Сгенерировать безопасные SQL-миграции для применения DDL в Trino + Iceberg.\n",
    "\n",
    "КРИТИЧЕСКИ ВАЖНЫЕ ПРАВИЛА СИНТАКСИСА:\n",
    "1. ВСЕ таблицы: {CATALOG}.<schema>.<table>\n",
    "2. ТОЛЬКО совместимый с Trino + Iceberg синтаксис\n",
    "3. НЕТ materialized views, индексов, деструктивных операций\n",
    "4. Формат файлов: используем WITH (format = 'PARQUET')\n",
    "\n",
    "РАЗРЕШЕННЫЕ ОПЕРАЦИИ МИГРАЦИИ:\n",
    "- INSERT INTO {CATALOG}.{TARGET_SCHEMA}.<table_new>\n",
    "  SELECT ... FROM {CATALOG}.{SOURCE_SCHEMA}.<table_old>\n",
    "- UPDATE/MERGE для целевых таблиц в {CATALOG}.{TARGET_SCHEMA}.*\n",
    "- ALTER TABLE только для {CATALOG}.{TARGET_SCHEMA}.* (например, свойства), если требуется для совместимости\n",
    "- Валидационные проверки допускаются только как часть миграционного шага (например, INSERT с агрегацией), но не отдельные SELECT\n",
    "\n",
    "ПОРЯДОК МИГРАЦИЙ:\n",
    "1. Перенос данных в новые таблицы (INSERT/UPDATE/MERGE) — цели: {CATALOG}.{TARGET_SCHEMA}.*\n",
    "2. Изменение свойств только новых таблиц (если необходимо)\n",
    "3. (Опционально) Легкая валидация, но без чистых SELECT\n",
    "\n",
    "ПРИМЕР ВАЛИДНОГО СИНТАКСИСА:\n",
    "INSERT INTO {CATALOG}.{TARGET_SCHEMA}.orders\n",
    "SELECT * FROM {CATALOG}.{SOURCE_SCHEMA}.orders;\n",
    "\n",
    "MERGE INTO {CATALOG}.{TARGET_SCHEMA}.orders t\n",
    "USING ({CATALOG}.{SOURCE_SCHEMA}.orders_delta) s\n",
    "ON t.id = s.id\n",
    "WHEN MATCHED THEN UPDATE SET amount = s.amount\n",
    "WHEN NOT MATCHED THEN INSERT (id, amount) VALUES (s.id, s.amount);\n",
    "\n",
    "ВЫВОД:\n",
    "ТОЛЬКО SQL-команды миграций, готовые к выполнению в Trino.\n",
    "Без комментариев, пояснений, текста.\n",
    "Каждая команда на новой строке.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38f9ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.671202Z",
     "start_time": "2025-10-19T15:46:24.666328Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_optimize_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Промпт для агента query_optimizer.\n",
    "    Агент получает новые DDL-запросы (new_ddl) и один исходный SQL-запрос (query),\n",
    "    и должен переписать данный запрос так, чтобы он был оптимизирован под новую структуру данных\n",
    "    (DDL), сохраняя исходную бизнес-логику и корректность результатов.\n",
    "    Работает в контексте Trino + Iceberg + S3.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Ты — SQL-оптимизатор для Trino + Iceberg + S3.\n",
    "\n",
    "[Контекст выполнения]\n",
    "- Catalog: {CATALOG}\n",
    "- Source schema (исходные таблицы): {SOURCE_SCHEMA}\n",
    "- Target schema (новая структура): {TARGET_SCHEMA}\n",
    "\n",
    "[Обязательные правила]\n",
    "- Полная квалификация всех таблиц: {CATALOG}.<schema>.<table>\n",
    "- В исходную схему {CATALOG}.{SOURCE_SCHEMA}.* НЕ писать и её не изменять\n",
    "  (запрещены ALTER/DROP/INSERT/UPDATE/MERGE/DELETE/CREATE VIEW/MATERIALIZED VIEW/GRANT/REVOKE).\n",
    "- Переписывай запрос так, чтобы он обращался ТОЛЬКО к {CATALOG}.{TARGET_SCHEMA}.* (новая структура).\n",
    "- Используй новые таблицы/колонки из new_ddl (денормализацию, партиции и т.д.), сохраняя бизнес-логику.\n",
    "- Синтаксис строго Trino + Iceberg; никаких materialized view, временных таблиц и т.п.\n",
    "\n",
    "ВХОД:\n",
    "- new_ddl: новые DDL таблиц (партиции, денормализации, свойства)\n",
    "- query: исходный SQL запрос для оптимизации\n",
    "\n",
    "ЦЕЛЬ:\n",
    "Переписать SQL запрос для использования новой структуры таблиц из DDL.\n",
    "Сохранить идентичную бизнес-логику и результат.\n",
    "\n",
    "ПРАВИЛА ОПТИМИЗАЦИИ:\n",
    "1. Используй новые таблицы и колонки из {CATALOG}.{TARGET_SCHEMA}.*\n",
    "2. Если в new_ddl есть денормализованные таблицы — убирай лишние JOIN и обращайся к денормализованным колонкам.\n",
    "3. Для партиционированных таблиц — добавляй/сохраняй фильтры по полям партиций (day()/month() и т.п.), если они присутствуют по смыслу в исходном запросе.\n",
    "4. Сохраняй все агрегации, фильтры, группировки и логику исходного запроса.\n",
    "5. Полная квалификация всех таблиц обязательна: {CATALOG}.{TARGET_SCHEMA}.<table> с алиасами.\n",
    "\n",
    "ЗАПРЕЩЕНО:\n",
    "- Менять бизнес-логику (WHERE, JOIN, GROUP BY, HAVING) сверх необходимой адаптации к новой структуре.\n",
    "- Materialized Views, индексы, временные таблицы, любые DDL или операции записи в {CATALOG}.{SOURCE_SCHEMA}.*.\n",
    "- Комментарии, пояснения, любой текст кроме одного SQL.\n",
    "\n",
    "АНТИ-ОШИБКИ (строго соблюдать):\n",
    "- Каждый источник в FROM/JOIN обязан иметь алиас; все колонки должны быть квалифицированы алиасом.\n",
    "- Нельзя ссылаться на несуществующие поля или алиасы; используй реальные поля исходной/новой таблицы.\n",
    "- Любой подзапрос/CTE — с алиасом: (...) AS sub.\n",
    "- GROUP BY: перечисляй только реально присутствующие в SELECT выражения/поля.\n",
    "- Скобки/запятые: без висячих запятых и лишних скобок.\n",
    "- Избегай недетерминизма: ORDER BY random() использовать только если требуется задачей.\n",
    "\n",
    "ВЫВОД: Только один оптимизированный SQL-запрос для Trino (SELECT/CTE), без комментариев.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3509797ce7388692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.835170Z",
     "start_time": "2025-10-19T15:46:24.828624Z"
    }
   },
   "outputs": [],
   "source": [
    "def critic_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Prompt for LLM‑critic: given original/optimized SQL and other data, return a fixed SQL.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Ты — LLM‑критик SQL для Trino + Iceberg.\n",
    "\n",
    "[Контекст выполнения]\n",
    "- Catalog: {CATALOG}\n",
    "- Source schema (исходные таблицы): {SOURCE_SCHEMA}\n",
    "- Target schema (новая структура): {TARGET_SCHEMA}\n",
    "\n",
    "[Обязательные правила формата]\n",
    "- Полная квалификация таблиц: {CATALOG}.<schema>.<table>.\n",
    "- Исходную схему {CATALOG}.{SOURCE_SCHEMA}.* НЕ изменять и не использовать для записи\n",
    "  (запрещены ALTER/DROP/INSERT/UPDATE/MERGE/DELETE/CREATE VIEW/MATERIALIZED VIEW/GRANT/REVOKE).\n",
    "- Оптимизированный запрос должен ссылаться ТОЛЬКО на {CATALOG}.{TARGET_SCHEMA}.*.\n",
    "  Если optimized использует старую схему — перепиши на {CATALOG}.{TARGET_SCHEMA}.* без изменения бизнес‑логики.\n",
    "- Никаких DDL/DML в ответе: только один SQL‑запрос (SELECT/CTE) или строка \"OK\".\n",
    "- Запрещено: materialized view, любая авторизация/GRANT/REVOKE.\n",
    "\n",
    "ВХОД (plain‑text блоки, без JSON):\n",
    "\n",
    "old_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "new_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "migrations:\n",
    "<CTAS/INSERT INTO ... SELECT ...>  (перенос из старой структуры в новую; опционально)\n",
    "\n",
    "original:\n",
    "<исходный SQL>\n",
    "\n",
    "optimized:\n",
    "<оптимизированный SQL>\n",
    "\n",
    "ЦЕЛЬ:\n",
    "- Самостоятельно найти и исправить ошибки в поле optimized, если они конечно присутствуют. Проведи собственную многошаговую проверку консистентности, используя при наличии DDL (ddl_old/ddl_new) как источник истины по колонкам и таблицам.\n",
    "- Исходный запрос (original) практически всегда валиден. Разрешено переиспользовать его конструкции (алиасы, выражения, подзапросы, группировки) во втором запросе, но обязательно с учётом уже выполненной оптимизации (например, денормализация, замена источников, добавление партиционных фильтров).\n",
    "- Если ошибок НЕТ — верни строку \"OK\" (без дополнительных слов).\n",
    "- Если ошибки ЕСТЬ — верни ТОЛЬКО один исправленный SQL‑запрос, без комментариев и текста.\n",
    "\n",
    "СТРОГИЙ ЧЕК‑ЛИСТ ПРОВЕРКИ:\n",
    "1) Алиасы: каждый источник в FROM/JOIN имеет алиас; все колонки квалифицированы правильным алиасом.\n",
    "2) Существование колонок: alias.column существует согласно DDL соответствующей таблицы; при денормализации используй новые таблицы из ddl_new. Если DDL отсутствуют — делай правки только по очевидным несоответствиям (алиасы/скобки/Group By), не придумывай колонки.\n",
    "3) JOIN‑ключи: ссылки только на колонки, реально присутствующие в соответствующих таблицах; не используй client_id у таблицы, где его нет.\n",
    "4) Агрегации: GROUP BY соответствует SELECT (все неагрегированные выражения перечислены), нет мусорных токенов (например, 'b1').\n",
    "5) Синтаксис: нет лишних/незакрытых скобок, висячих запятых. Синтаксис строго Trino.\n",
    "6) Детерминизм: избегай ORDER BY random() без явной необходимости.\n",
    "7) Схемы: оптимизированный запрос использует ТОЛЬКО {CATALOG}.{TARGET_SCHEMA}.*; никаких ссылок на {CATALOG}.{SOURCE_SCHEMA}.*.\n",
    "\n",
    "ВЫВОД:\n",
    "- Если ошибок нет: ровно \"OK\".\n",
    "- Если есть ошибки: только исправленный SQL.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e40047fc7535e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:25.013185Z",
     "start_time": "2025-10-19T15:46:25.008447Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Prompt for LLM‑judge: decide if optimized preserves original semantics from structure only.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Ты — LLM‑судья эквивалентности SQL (Trino).\n",
    "\n",
    "[Контекст выполнения]\n",
    "- Catalog: {CATALOG}\n",
    "- Source schema (исходные таблицы): {SOURCE_SCHEMA}\n",
    "- Target schema (новая структура): {TARGET_SCHEMA}\n",
    "\n",
    "[Обязательные правила формата]\n",
    "- Полная квалификация таблиц: {CATALOG}.<schema>.<table>.\n",
    "- Исходную схему {CATALOG}.{SOURCE_SCHEMA}.* НЕ изменять и не использовать для записи\n",
    "  (запрещены ALTER/DROP/INSERT/UPDATE/MERGE/DELETE/CREATE VIEW/MATERIALIZED VIEW/GRANT/REVOKE).\n",
    "- Оптимизированный запрос должен ссылаться ТОЛЬКО на {CATALOG}.{TARGET_SCHEMA}.* (новая структура).\n",
    "- Запрещено: materialized view, любая авторизация/GRANT/REVOKE.\n",
    "- Если optimized ссылается на старую схему, но семантика идентична — перепиши на {CATALOG}.{TARGET_SCHEMA}.* без изменения бизнес‑логики.\n",
    "\n",
    "ВХОД (plain‑text блоки, без JSON):\n",
    "\n",
    "old_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "new_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "migrations:\n",
    "<CTAS/INSERT INTO ... SELECT ...>  (перенос из старой в новую; опционально)\n",
    "\n",
    "original:\n",
    "<исходный SQL>\n",
    "\n",
    "optimized:\n",
    "<оптимизированный SQL>\n",
    "\n",
    "ЗАДАЧА:\n",
    "- По структуре SQL (без выполнения) оцени, сохраняет ли optimized бизнес‑логику original (те же агрегаты, фильтры, соединения, проекции) с допустимыми адаптациями под оптимизацию (денормализация, партиционные фильтры и т. п.).\n",
    "- Учитывай DDL/миграции: если optimized читает данные из новых таблиц (по CTAS/INSERT), интерпретируй соответствия полей между старой и новой структурой.\n",
    "- При необходимости скорректируй optimized (только SQL) так, чтобы он использовал {CATALOG}.{TARGET_SCHEMA}.* и сохранял семантику original.\n",
    "\n",
    "СТРОГИЙ ЧЕК‑ЛИСТ:\n",
    "1) Схемы: optimized использует ТОЛЬКО {CATALOG}.{TARGET_SCHEMA}.*; отсутствие ссылок на {CATALOG}.{SOURCE_SCHEMA}.*.\n",
    "2) Алиасы: у каждого источника в FROM/JOIN есть алиас; колонки квалифицированы корректным алиасом.\n",
    "3) Поля: alias.column существуют согласно DDL соответствующих таблиц; при денормализации используй новые таблицы из new_ddl.\n",
    "4) JOIN‑ключи: соответствуют реальным полям обеих таблиц.\n",
    "5) Агрегации: GROUP BY согласован с SELECT; без мусорных токенов.\n",
    "6) Синтаксис: нет лишних/незакрытых скобок, висячих запятых; синтаксис Trino.\n",
    "7) Детерминизм: избегай ORDER BY random() без явной необходимости.\n",
    "\n",
    "ВЫВОД:\n",
    "- Если ошибок нет: ровно \"OK\".\n",
    "- Если есть проблема (семантика/структура/синтаксис/схема): верни полный исправленный SQL (один запрос, без комментариев/объяснений).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ca3ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:06.751174Z",
     "start_time": "2025-10-19T15:46:28.774907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Краткое резюме\n",
      "\n",
      "Наиболее часто используемые таблицы в запросах:  \n",
      "- `s_client_personal_info` — используется в 4 из 5 запросов, содержит данные о клиентах, включая возраст и источник регистрации.  \n",
      "- `l_payment_client` — участвует в 4 запросах, связывает платежи с клиентами.  \n",
      "- `h_client` — участвует в 3 запросах, содержит информацию о клиенте, включая дату создания.  \n",
      "- `s_client_geo_info` — участвует в 4 запросах, содержит географические данные клиентов.  \n",
      "- `l_excursion_payment` и `l_quest_payment` — участвуют в 2 запросах, отражают данные о покупках экскурсий и квестов.  \n",
      "- `s_excursion_geo_info` — участвует в 1 запросе, содержит информацию о географии экскурсий.\n",
      "\n",
      "Часто используемые поля:  \n",
      "- `client_id`, `payment_id`, `id` — ключевые поля для соединений.  \n",
      "- `payment_dt`, `created_at` — даты, часто участвуют в фильтрах и агрегациях.  \n",
      "- `region`, `city`, `language` — географические поля, часто используются в GROUP BY и фильтрах.  \n",
      "- `age` — используется для группировки по возрастным категориям.  \n",
      "- `registration_source` — источник регистрации, часто используется в агрегациях.\n",
      "\n",
      "### Анализ соединений\n",
      "\n",
      "Часто используются следующие соединения между таблицами:  \n",
      "- `l_payment_client` ↔ `h_client` по `client_id` — используется в 4 запросах.  \n",
      "- `l_payment_client` ↔ `s_client_geo_info` по `client_id` — участвует в 4 запросах.  \n",
      "- `l_excursion_payment` ↔ `l_payment_client` по `payment_id` — участвует в 2 запросах.  \n",
      "- `l_quest_payment` ↔ `l_payment_client` по `payment_id` — участвует в 2 запросах.  \n",
      "- `s_client_personal_info` ↔ `h_client` по `client_id` — участвует в 2 запросах.  \n",
      "- `s_client_personal_info` ↔ `s_client_geo_info` по `client_id` — участвует в 2 запросах.  \n",
      "\n",
      "Также в одном запросе используется `CROSS JOIN` с таблицей `h_client`, что может быть источником нагрузки.\n",
      "\n",
      "### Анализ фильтров\n",
      "\n",
      "Часто используются следующие поля в WHERE/ON:  \n",
      "- `payment_dt` — фильтрация по дате платежа, особенно в запросах с агрегацией по кварталам и месяцам.  \n",
      "- `client_id`, `payment_id` — ключевые поля для соединений.  \n",
      "- `created_at` — используется в фильтрах и агрегациях, особенно в запросах с датами регистрации.  \n",
      "- `first_purchase_date` — используется для определения покупателей.  \n",
      "- `registration_source` — используется в фильтрах и группировках.  \n",
      "\n",
      "В одном запросе используется `date_trunc('month', ...)` и `date_trunc('quarter', ...)` — это может быть оптимизировано через партиционирование по дате.\n",
      "\n",
      "### Анализ операций\n",
      "\n",
      "Часто встречаются следующие операции:  \n",
      "- `GROUP BY` — используется в 5 запросах, часто по полям `region`, `city`, `language`, `month`, `age_group`, `registration_source`.  \n",
      "- `JOIN` — используется в 5 запросах, включая LEFT JOIN и CROSS JOIN.  \n",
      "- `COUNT(*)`, `COUNT(ep.excursion_id)`, `COUNT(qp.quest_id)` — агрегации, часто с условными подсчетами.  \n",
      "- `CASE WHEN` — используется для группировки по возрастным категориям и кварталам.  \n",
      "- `ORDER BY` — применяется в 4 запросах, часто с агрегациями и сортировкой по количеству покупок или конверсии.  \n",
      "- `UNION ALL` — используется в 2 запросах для объединения результатов по разным типам продуктов.  \n",
      "- `ROW_NUMBER()` и `OVER (PARTITION BY ...)` — применяется в одном запросе для нумерации строк.  \n",
      "- `WITH` — используется в одном запросе для создания CTE, что может быть оптимизировано через партиционирование или денормализацию.\n",
      "\n",
      "### Конструкции, создающие нагрузку\n",
      "\n",
      "- `CROSS JOIN` с ограничением `LIMIT 10` — может быть источником нагрузки, особенно если таблица `h_client` большая.  \n",
      "- `date_trunc('month', ...)` и `date_trunc('quarter', ...)` — могут быть оптимизированы через партиционирование по дате.  \n",
      "- `ORDER BY random()` — может быть тяжелой операцией, особенно при большом объеме данных.  \n",
      "- `SELECT AVG(...) FROM big b2 WHERE b2.region = b1.region` — подзапросы, которые могут быть оптимизированы через предварительные агрегации или партиционирование.  \n",
      "- `ROW_NUMBER() OVER (PARTITION BY ...)` — может быть оптимизирован через сортировку и партиционирование.\n",
      "\n",
      "### Возможные улучшения на уровне DDL\n",
      "\n",
      "- **Партиционирование по дате**:  \n",
      "  Для таблиц `l_payment_client`, `h_client`, `l_excursion_payment`, `l_quest_payment` можно рассмотреть партиционирование по `payment_dt` или `created_at`, чтобы ускорить фильтрацию по датам.  \n",
      "  Для `s_client_geo_info` и `s_excursion_geo_info` можно рассмотреть партиционирование по `region` или `city`, если данные распределены по географическим зонам.\n",
      "\n",
      "- **Сортировка (Sort Order)**:  \n",
      "  Для таблиц, где часто происходит фильтрация по `client_id`, `payment_id`, `id`, можно рассмотреть сортировку по этим полям при создании таблиц, чтобы ускорить поиск и соединения.\n",
      "\n",
      "- **Денормализация**:  \n",
      "  В запросах часто используются данные из нескольких таблиц, что может быть оптимизировано через денормализацию. Например, можно объединить `s_client_personal_info` и `s_client_geo_info` в одну таблицу, если данные не слишком объемные и не требуют частого обновления.\n",
      "\n",
      "- **Использование сортировки по ключам в таблицах**:  \n",
      "  Для таблиц, где часто происходит `JOIN` по `client_id`, можно рассмотреть сортировку по `client_id` при создании таблиц, чтобы ускорить соединения.\n",
      "\n",
      "- **Ограничение на количество строк в CTE**:  \n",
      "  В одном запросе используется `UNION ALL` с повторением данных, что может быть оптимизировано через партиционирование или предварительную агрегацию.\n",
      "\n",
      "- **Упрощение подзапросов**:  \n",
      "  В запросе с `SELECT AVG(...) FROM big b2 WHERE b2.region = b1.region` можно рассмотреть предварительную агрегацию данных по регионам, чтобы избежать повторных вычислений.\n",
      "\n",
      "- **Оптимизация `ORDER BY random()`**:  \n",
      "  Если `ORDER BY random()` используется для выборки случайных строк, можно рассмотреть использование `TABLESAMPLE` или предварительное создание случайных подмножеств данных.\n",
      "\n",
      "### Краткое резюме\n",
      "\n",
      "Наиболее часто используемые таблицы в запросах:  \n",
      "- `quests.public.h_author`, `quests.public.l_excursion_author`, `quests.public.l_excursion_payment`, `quests.public.s_payment_info` — участвуют в первом запросе, где происходит объединение данных об авторах, экскурсиях и платежах.  \n",
      "- `quests.public.l_payment_client`, `quests.public.s_client_personal_info`, `quests.public.s_payment_info` — используются в запросах, связанных с анализом клиентов и их покупок.  \n",
      "- `quests.public.l_excursion_payment`, `quests.public.l_excursion_category`, `quests.public.h_category`, `quests.public.s_payment_info` — участвуют в запросах по категориям и анализу продаж.  \n",
      "- `quests.public.h_episode`, `quests.public.s_episode_completion_info`, `quests.public.l_quest_episode`, `quests.public.h_quest` — используются в запросе по завершению эпизодов и квестов.\n",
      "\n",
      "Часто используемые поля:  \n",
      "- `id`, `client_id`, `payment_id`, `excursion_id`, `quest_id`, `episode_id`, `category_id` — ключевые поля для соединений.  \n",
      "- `amount`, `is_repeat_purchase`, `time_spent`, `loyalty_level` — поля, по которым выполняются агрегации и фильтрации.\n",
      "\n",
      "### Анализ соединений\n",
      "\n",
      "Часто используются следующие соединения между таблицами:  \n",
      "- `h_author` ↔ `l_excursion_author` ↔ `l_excursion_payment` ↔ `s_payment_info` — для анализа данных об авторах и их доходах.  \n",
      "- `l_payment_client` ↔ `s_client_personal_info` ↔ `s_payment_info` — для анализа клиентов и их покупок.  \n",
      "- `l_excursion_payment` ↔ `l_excursion_category` ↔ `h_category` ↔ `s_payment_info` — для анализа продаж по категориям.  \n",
      "- `h_episode` ↔ `s_episode_completion_info` ↔ `l_quest_episode` ↔ `h_quest` — для анализа завершения эпизодов и квестов.\n",
      "\n",
      "### Анализ фильтров\n",
      "\n",
      "Часто используются поля в условиях `WHERE` и `ON`:  \n",
      "- `id`, `client_id`, `payment_id`, `excursion_id`, `quest_id`, `episode_id` — ключевые поля для соединений и фильтрации.  \n",
      "- `amount` — используется в агрегациях и сравнениях.  \n",
      "- `is_repeat_purchase` — используется в условиях для выделения повторных покупок.  \n",
      "- `loyalty_level` — используется в группировке и фильтрации клиентов.\n",
      "\n",
      "### Анализ операций\n",
      "\n",
      "Часто выполняются следующие операции:  \n",
      "- `GROUP BY` — используется для агрегации данных по клиентам, категориям, авторам и т. д.  \n",
      "- `JOIN` — применяется для объединения данных из нескольких таблиц.  \n",
      "- `ORDER BY` — используется для сортировки результатов по количеству, среднему значению и т. д.  \n",
      "- `DISTINCT`, `COUNT`, `AVG`, `SUM`, `MAX`, `MIN` — применяются для вычисления статистик.  \n",
      "- `ROW_NUMBER()` и `OVER (PARTITION BY ...)` — используются для ранжирования и оконных функций.  \n",
      "- `CASE WHEN` — применяется для сегментации данных (например, по количеству покупок).  \n",
      "- `UNION ALL` — используется для объединения результатов по экскурсиям и квестам.\n",
      "\n",
      "### Потенциальные узкие места и нагрузка\n",
      "\n",
      "- Запрос с `CROSS JOIN` и `LIMIT 500000` в первом запросе может создавать высокую нагрузку, особенно при большом количестве записей.  \n",
      "- Использование `ORDER BY random()` в первом и последнем запросах может быть неэффективным, особенно при большом объеме данных.  \n",
      "- Наличие `ROW_NUMBER()` с `ORDER BY random()` может быть затратным, особенно при большом количестве строк.  \n",
      "- В запросах с `UNION ALL` и `GROUP BY` может быть полезно учитывать размеры таблиц и оптимизировать соединения.\n",
      "\n",
      "### Возможные улучшения на уровне DDL\n",
      "\n",
      "- **Партиционирование**:  \n",
      "  - По дате (если данные имеют временные метки) — особенно для таблиц `s_payment_info`, `s_episode_completion_info`, `l_payment_client`.  \n",
      "  - По `client_id` или `category_id` — для таблиц с высокой степенью фильтрации.  \n",
      "\n",
      "- **Сортировка (Sort Order)**:  \n",
      "  - Для таблиц с частыми `JOIN` по `id`, `client_id`, `payment_id` — можно рассмотреть сортировку по этим полям.  \n",
      "\n",
      "- **Денормализация**:  \n",
      "  - Если часто выполняются запросы с `JOIN` между `l_payment_client` и `s_payment_info`, можно рассмотреть объединение данных в одну таблицу или создание материализованных представлений (в рамках ограничений, не используем их).  \n",
      "\n",
      "- **Использование `ORDER BY` в DDL**:  \n",
      "  - Для таблиц, где часто используется `ORDER BY` по определенным полям, можно рассмотреть сортировку данных по этим полям при загрузке.  \n",
      "\n",
      "- **Ограничение объема данных**:  \n",
      "  - В запросе с `CROSS JOIN` и `LIMIT` можно рассмотреть возможность уменьшения объема данных на этапе загрузки или фильтрации.  \n",
      "\n",
      "- **Оптимизация `ORDER BY random()`**:  \n",
      "  - Если `ORDER BY random()` используется для выборки случайных записей, можно рассмотреть альтернативные способы, например, использование `TABLESAMPLE` или предварительное создание случайных подмножеств данных.  \n",
      "\n",
      "- **Использование `VALUES` в запросах**:  \n",
      "  - В запросе с `CROSS JOIN (VALUES 1,2)` можно рассмотреть возможность замены на более эффективные конструкции, если объем данных позволяет.  \n",
      "\n",
      "- **Оптимизация `UNION ALL`**:  \n",
      "  - Если таблицы `l_excursion_payment` и `l_quest_payment` имеют схожую структуру, можно рассмотреть объединение их в одну таблицу с дополнительным полем типа `type` (excursion/quest).  \n",
      "\n",
      "Таким образом, наиболее эффективные улучшения на уровне DDL будут связаны с партиционированием, сортировкой и возможной денормализацией, особенно для таблиц с высокой степенью повторения данных и частыми `JOIN` и `GROUP BY`.\n",
      "\n",
      "### Краткое резюме\n",
      "\n",
      "Наиболее часто участвуют таблицы: `l_payment_client`, `s_payment_info` и `l_payment_promo`. Основные поля, используемые в запросе: `payment_dt`, `payment_id`, `promo_id`, `amount`. Запрос выполняется 34 раза, при этом среднее время выполнения одного запроса составляет 10 единиц времени (например, секунд). Это указывает на высокую частоту обращения к данным, что требует оптимизации.\n",
      "\n",
      "### Анализ соединений\n",
      "\n",
      "Таблицы `l_payment_client` и `s_payment_info` соединяются по полю `payment_id`, что является ключевым для получения информации о платежах. Далее, таблица `l_payment_promo` присоединяется LEFT JOIN по тому же полю `payment_id`, чтобы определить, был ли применен промо-код. Соединения происходят по одному ключу, что позволяет предположить, что данные в таблицах могут быть эффективно сгруппированы по `payment_id`.\n",
      "\n",
      "### Анализ фильтров\n",
      "\n",
      "В запросе используется поле `payment_dt` в функции `date_trunc('month', ...)` для группировки по месяцам. Также в фильтрах участвуют `promo_id` и `amount`. В WHERE-условиях не используются явные фильтры, но в логике запроса применяется условие `IF(pp.promo_id IS NOT NULL, ...)` для вычисления среднего чека по промо-продажам и обычным продажам. Это указывает на необходимость оптимизации поиска по `promo_id` и `payment_id`.\n",
      "\n",
      "### Анализ операций\n",
      "\n",
      "Запрос включает следующие операции:\n",
      "- `GROUP BY` по дате (месяц), что требует агрегации данных.\n",
      "- `COUNT(*)`, `COUNT(pp.promo_id)`, `SUM(pi.amount)` — агрегации по количеству и сумме.\n",
      "- `AVG()` с условием — вычисление среднего чека с фильтрацией по промо-акциям.\n",
      "- `ORDER BY` по месяцу — сортировка результатов.\n",
      "- `LEFT JOIN` — для получения информации о промо-акциях, что может быть затратным при большом количестве записей.\n",
      "\n",
      "### Возможные улучшения на уровне DDL\n",
      "\n",
      "1. **Партиционирование по дате**: Поскольку в запросе используется `payment_dt` для группировки по месяцам, можно рассмотреть партиционирование таблиц `l_payment_client` и `s_payment_info` по полю `payment_dt`. Это позволит ускорить фильтрацию и агрегацию по датам.\n",
      "\n",
      "2. **Сортировка данных по ключам**: Для таблиц `l_payment_client` и `l_payment_promo` рекомендуется рассмотреть сортировку по `payment_id`, так как именно по этому полю происходит соединение. Это может ускорить JOIN-операции.\n",
      "\n",
      "3. **Денормализация данных**: Если таблица `l_payment_promo` содержит небольшое количество записей по сравнению с `l_payment_client`, можно рассмотреть возможность включения поля `promo_id` в таблицу `l_payment_client` или `s_payment_info`, чтобы избежать LEFT JOIN и упростить логику запроса.\n",
      "\n",
      "4. **Использование сортированных колонок в GROUP BY**: Если `payment_dt` часто используется в GROUP BY, можно рассмотреть сортировку данных по этому полю, чтобы ускорить агрегацию.\n",
      "\n",
      "5. **Оптимизация структуры таблицы `l_payment_promo`**: Если `promo_id` встречается редко, возможно, стоит рассмотреть его вынос в отдельную таблицу или использование более компактного формата хранения, чтобы уменьшить объем данных при JOIN.\n",
      "\n",
      "Таким образом, запрос требует оптимизации на уровне структуры данных, особенно в части партиционирования, сортировки и возможной денормализации.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def get_answer_async(agent, querys_batch, retry_nums=3):\n",
    "    for _ in range(retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content='\\n\\n'.join(querys_batch)\n",
    "            )\n",
    "            response = await agent.ainvoke({'messages': prompt})\n",
    "            return response[\"messages\"][-1].content\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Retrying...\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_CALLS)\n",
    "\n",
    "async def bounded_get_answer_async(agent, batch):\n",
    "    async with semaphore:\n",
    "        return await get_answer_async(agent, batch)\n",
    "\n",
    "query_analize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=query_analize_prompt(),\n",
    ")\n",
    "\n",
    "querys_batch_prompt = [f\"sql-запрос: {i['query']}, количество выполнения:{i['runquantity']}, колличество затраченного времени при едином выполнении запроса{i['executiontime']}\" for i in data['queries']]\n",
    "\n",
    "batches = [\n",
    "    querys_batch_prompt[5 * i : 5 * (i + 1)]\n",
    "    for i in range(len(querys_batch_prompt) // 5 + int(len(querys_batch_prompt) % 5 != 0))\n",
    "]\n",
    "\n",
    "tasks = [bounded_get_answer_async(query_analize_agent, batch) for batch in batches]\n",
    "\n",
    "results = await asyncio.gather(*tasks)\n",
    "\n",
    "analize = \"\\n\\n\".join(results)\n",
    "print(analize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfea92be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:38.467012Z",
     "start_time": "2025-10-19T15:48:06.829524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Сводный отчёт по анализу SQL-запросов\n",
      "\n",
      "#### 1. \"Hot\" таблицы (наиболее часто упоминаемые в тяжёлых операциях)\n",
      "- `l_payment_client` — участвует в 5 запросах, используется в JOIN и агрегациях.\n",
      "- `s_client_personal_info` — участвует в 4 запросах, часто в соединениях и GROUP BY.\n",
      "- `s_client_geo_info` — участвует в 4 запросах, используется в JOIN и фильтрах.\n",
      "- `h_client` — участвует в 3 запросах, часто в соединениях.\n",
      "- `l_excursion_payment` и `l_quest_payment` — участвуют в 2 запросах, используются в JOIN.\n",
      "- `s_excursion_geo_info` — участвует в 1 запросе, содержит географию экскурсий.\n",
      "\n",
      "#### 2. Частые пары JOIN и их ключи\n",
      "- `l_payment_client` ↔ `h_client` по `client_id` — 4 раза.\n",
      "- `l_payment_client` ↔ `s_client_geo_info` по `client_id` — 4 раза.\n",
      "- `l_excursion_payment` ↔ `l_payment_client` по `payment_id` — 2 раза.\n",
      "- `l_quest_payment` ↔ `l_payment_client` по `payment_id` — 2 раза.\n",
      "- `s_client_personal_info` ↔ `h_client` по `client_id` — 2 раза.\n",
      "- `s_client_personal_info` ↔ `s_client_geo_info` по `client_id` — 2 раза.\n",
      "\n",
      "#### 3. Часто используемые фильтры (WHERE/ON)\n",
      "- `payment_dt` — используется в фильтрах и агрегациях по датам.\n",
      "- `client_id`, `payment_id` — ключевые поля для соединений.\n",
      "- `created_at` — используется в фильтрах и агрегациях.\n",
      "- `first_purchase_date` — используется в фильтрах.\n",
      "- `registration_source` — используется в фильтрах и группировках.\n",
      "- `amount` — используется в агрегациях и сравнениях.\n",
      "- `is_repeat_purchase` — используется в условиях.\n",
      "- `loyalty_level` — используется в группировке и фильтрации.\n",
      "\n",
      "#### 4. Частые тяжёлые операции\n",
      "- `GROUP BY` — используется в 5 запросах, часто по `region`, `city`, `language`, `month`, `age_group`, `registration_source`.\n",
      "- `JOIN` — используется в 5 запросах, включая LEFT JOIN и CROSS JOIN.\n",
      "- `ORDER BY` — применяется в 4 запросах, часто с агрегациями.\n",
      "- `COUNT(*)`, `COUNT(ep.excursion_id)`, `COUNT(qp.quest_id)` — агрегации.\n",
      "- `CASE WHEN` — используется для группировки по возрастным категориям и кварталам.\n",
      "- `UNION ALL` — используется в 2 запросах.\n",
      "- `ROW_NUMBER()` и `OVER (PARTITION BY ...)` — применяется в 1 запросе.\n",
      "- `CROSS JOIN` — используется в 1 запросе с ограничением `LIMIT`, может быть источником нагрузки.\n",
      "\n",
      "#### 5. Повторяющиеся паттерны для DDL-изменений\n",
      "- Частые соединения по `client_id` и `payment_id` — указывают на необходимость сортировки по этим полям.\n",
      "- Частое использование `payment_dt` в фильтрах и агрегациях — указывает на необходимость партиционирования по дате.\n",
      "- Частые `GROUP BY` по `region`, `city`, `language` — указывают на возможность партиционирования или сортировки по этим полям.\n",
      "- Повторяющиеся `JOIN` между `l_payment_client` и `s_payment_info` — указывает на возможность денормализации или предварительной агрегации.\n",
      "\n",
      "#### 6. Рекомендации уровня DDL\n",
      "\n",
      "##### 1. **Партиционирование по дате**\n",
      "- **Цель**: Ускорить фильтрацию по датам.\n",
      "- **Таблицы**: `l_payment_client`, `h_client`, `l_excursion_payment`, `l_quest_payment`, `s_payment_info`, `s_episode_completion_info`.\n",
      "- **Поле**: `payment_dt`, `created_at`.\n",
      "- **Причина**: Частое использование `payment_dt` в фильтрах и агрегациях.\n",
      "- **Impact**: HIGH.\n",
      "- **Метрики**: data_volume, query_frequency, avg_runtime.\n",
      "\n",
      "##### 2. **Сортировка (Sort Order) по ключевым полям**\n",
      "- **Цель**: Ускорить соединения и поиск.\n",
      "- **Таблицы**: `l_payment_client`, `l_payment_promo`, `s_client_personal_info`, `s_client_geo_info`.\n",
      "- **Поля**: `client_id`, `payment_id`, `id`.\n",
      "- **Причина**: Частые JOIN по этим полям.\n",
      "- **Impact**: MEDIUM.\n",
      "- **Метрики**: query_frequency, avg_runtime.\n",
      "\n",
      "##### 3. **Денормализация**\n",
      "- **Цель**: Уменьшить количество JOIN и ускорить запросы.\n",
      "- **Таблицы**: `s_client_personal_info`, `s_client_geo_info`.\n",
      "- **Причина**: Частые соединения между этими таблицами.\n",
      "- **Impact**: MEDIUM.\n",
      "- **Метрики**: query_frequency, cardinality.\n",
      "\n",
      "##### 4. **Сортировка по полям в GROUP BY**\n",
      "- **Цель**: Ускорить агрегации.\n",
      "- **Таблицы**: `l_payment_client`, `s_payment_info`.\n",
      "- **Поля**: `payment_dt`, `region`, `city`, `language`.\n",
      "- **Причина**: Частое использование этих полей в GROUP BY.\n",
      "- **Impact**: MEDIUM.\n",
      "- **Метрики**: data_volume, query_frequency.\n",
      "\n",
      "##### 5. **Оптимизация структуры таблицы `l_payment_promo`**\n",
      "- **Цель**: Уменьшить объем данных при JOIN.\n",
      "- **Таблицы**: `l_payment_promo`.\n",
      "- **Причина**: Частое использование `promo_id` в LEFT JOIN.\n",
      "- **Impact**: LOW.\n",
      "- **Метрики**: data_volume, cardinality.\n",
      "\n",
      "##### 6. **Оптимизация `ORDER BY random()`**\n",
      "- **Цель**: Уменьшить нагрузку при случайной выборке.\n",
      "- **Таблицы**: Все, где используется `ORDER BY random()`.\n",
      "- **Причина**: Может быть тяжелой операцией.\n",
      "- **Impact**: LOW.\n",
      "- **Метрики**: data_volume, avg_runtime.\n",
      "\n",
      "##### 7. **Оптимизация `CROSS JOIN`**\n",
      "- **Цель**: Уменьшить нагрузку при CROSS JOIN.\n",
      "- **Таблицы**: `h_client`.\n",
      "- **Причина**: CROSS JOIN с ограничением `LIMIT` может быть источником нагрузки.\n",
      "- **Impact**: MEDIUM.\n",
      "- **Метрики**: data_volume, query_frequency.\n",
      "\n",
      "##### 8. **Оптимизация `UNION ALL`**\n",
      "- **Цель**: Уменьшить дублирование данных.\n",
      "- **Таблицы**: `l_excursion_payment`, `l_quest_payment`.\n",
      "- **Причина**: Используется в UNION ALL.\n",
      "- **Impact**: LOW.\n",
      "- **Метрики**: data_volume, query_frequency.\n",
      "\n",
      "#### Приоритет изменений (по порядку):\n",
      "1. **Партиционирование по дате** — высокий приоритет.\n",
      "2. **Сортировка по ключевым полям** — средний приоритет.\n",
      "3. **Денормализация** — средний приоритет.\n",
      "4. **Оптимизация `ORDER BY random()` и `CROSS JOIN`** — низкий приоритет.\n",
      "5. **Оптимизация `UNION ALL` и структуры `l_payment_promo`** — низкий приоритет.\n",
      "\n",
      "#### Замечания:\n",
      "- В запросах часто используется `date_trunc('month', ...)` и `date_trunc('quarter', ...)` — можно оптимизировать через партиционирование.\n",
      "- В одном запросе используется `CROSS JOIN` с ограничением `LIMIT` — потенциальный источник нагрузки.\n",
      "- В запросах часто используются `ROW_NUMBER()` и `OVER (PARTITION BY ...)` — можно оптимизировать через сортировку и партиционирование.\n",
      "- В запросах часто используются `UNION ALL` — можно рассмотреть объединение таблиц с дополнительным полем типа `type`.\n"
     ]
    }
   ],
   "source": [
    "# summaraize analize from querys\n",
    "\n",
    "def get_answer(agent, querys_analize):\n",
    "    for _ in range(defolt_retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content= f\"Анализ полученных sql запросов{querys_analize}\"\n",
    "            )\n",
    "            return agent.invoke({'messages': prompt})[\"messages\"][-1].content\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Do retry\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    \n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "query_summarize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=query_summarize_prompt(),\n",
    ")\n",
    "\n",
    "query_summarize = get_answer(query_summarize_agent, analize)\n",
    "print(query_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33a590a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:41.487952Z",
     "start_time": "2025-10-19T15:48:38.478128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE SCHEMA quests.public_v2;\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_author AS SELECT * FROM quests.public.h_author WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_category AS SELECT * FROM quests.public.h_category WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_client AS SELECT * FROM quests.public.h_client WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_episode AS SELECT * FROM quests.public.h_episode WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_episode_completion AS SELECT * FROM quests.public.h_episode_completion WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_excursion AS SELECT * FROM quests.public.h_excursion WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_payment AS SELECT * FROM quests.public.h_payment WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_promo AS SELECT * FROM quests.public.h_promo WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_quest AS SELECT * FROM quests.public.h_quest WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_review AS SELECT * FROM quests.public.h_review WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.h_session AS SELECT * FROM quests.public.h_session WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_author_quest AS SELECT * FROM quests.public.l_author_quest WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_client_episode_completion AS SELECT * FROM quests.public.l_client_episode_completion WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_client_review AS SELECT * FROM quests.public.l_client_review WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_client_session AS SELECT * FROM quests.public.l_client_session WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_excursion_author AS SELECT * FROM quests.public.l_excursion_author WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_excursion_category AS SELECT * FROM quests.public.l_excursion_category WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_excursion_payment AS SELECT * FROM quests.public.l_excursion_payment WITH (format = 'PARQUET', partitioning = ARRAY['day(payment_dt)']);\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_excursion_review AS SELECT * FROM quests.public.l_excursion_review WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_payment_client AS SELECT * FROM quests.public.l_payment_client WITH (format = 'PARQUET', partitioning = ARRAY['day(payment_dt)']);\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_payment_promo AS SELECT * FROM quests.public.l_payment_promo WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_quest_category AS SELECT * FROM quests.public.l_quest_category WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_quest_episode AS SELECT * FROM quests.public.l_quest_episode WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_quest_payment AS SELECT * FROM quests.public.l_quest_payment WITH (format = 'PARQUET', partitioning = ARRAY['day(payment_dt)']);\n",
      "\n",
      "CREATE TABLE quests.public_v2.l_quest_review AS SELECT * FROM quests.public.l_quest_review WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.ref_categories AS SELECT * FROM quests.public.ref_categories WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.ref_cities AS SELECT * FROM quests.public.ref_cities WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.ref_genres AS SELECT * FROM quests.public.ref_genres WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.ref_loyalty_levels AS SELECT * FROM quests.public.ref_loyalty_levels WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.ref_professions AS SELECT * FROM quests.public.ref_professions WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.ref_sources AS SELECT * FROM quests.public.ref_sources WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_author_geo_info AS SELECT * FROM quests.public.s_author_geo_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_author_personal_info AS SELECT * FROM quests.public.s_author_personal_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_category_info AS SELECT * FROM quests.public.s_category_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_client_geo_info AS SELECT * FROM quests.public.s_client_geo_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_client_personal_info AS SELECT * FROM quests.public.s_client_personal_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_episode_completion_info AS SELECT * FROM quests.public.s_episode_completion_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_episode_info AS SELECT * FROM quests.public.s_episode_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_excursion_geo_info AS SELECT * FROM quests.public.s_excursion_geo_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_excursion_info AS SELECT * FROM quests.public.s_excursion_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_payment_info AS SELECT * FROM quests.public.s_payment_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_promo_info AS SELECT * FROM quests.public.s_promo_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_quest_info AS SELECT * FROM quests.public.s_quest_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_review_info AS SELECT * FROM quests.public.s_review_info WITH (format = 'PARQUET');\n",
      "\n",
      "CREATE TABLE quests.public_v2.s_session_info AS SELECT * FROM quests.public.s_session_info WITH (format = 'PARQUET');\n"
     ]
    }
   ],
   "source": [
    "# DDL optimize by analize and orig_ddl\n",
    "\n",
    "def clean_sql(s: str) -> str:\n",
    "    # Убираем возможные кодовые блоки и обрезаем пробелы\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s2 = s.strip()\n",
    "    # частые варианты окружения\n",
    "    s2 = s2.replace(\"```sql\", \"```\")\n",
    "    if s2.startswith(\"```\") and s2.endswith(\"```\"):\n",
    "        s2 = s2[3:-3].strip()\n",
    "    return s2\n",
    "\n",
    "def get_answer(agent, query_summarize, ddl_orig):\n",
    "    for _ in range(defolt_retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content=(\n",
    "                    f\"[Context]\\n\"\n",
    "                    f\"Catalog: {CATALOG}\\n\"\n",
    "                    f\"Source schema: {SOURCE_SCHEMA}\\n\"\n",
    "                    f\"Target schema: {TARGET_SCHEMA}\\n\\n\"\n",
    "                    f\"Анализ полученных sql запросов:\\n{query_summarize}\\n\\n\"\n",
    "                    f\"Это оригинальные DDL запросы:\\n{DDL_querys}\"\n",
    "                )\n",
    "            )\n",
    "            resp = agent.invoke({'messages': prompt})[\"messages\"][-1].content\n",
    "            return clean_sql(resp)\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Do retry\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "ddl_optimize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=ddl_optim_prompt(),\n",
    ")\n",
    "\n",
    "New_DDL = get_answer(ddl_optimize_agent, query_summarize, DDL_querys)\n",
    "print(New_DDL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6b1431a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:44.875204Z",
     "start_time": "2025-10-19T15:48:41.495102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO quests.public_v2.h_author SELECT * FROM quests.public.h_author;\n",
      "INSERT INTO quests.public_v2.h_category SELECT * FROM quests.public.h_category;\n",
      "INSERT INTO quests.public_v2.h_client SELECT * FROM quests.public.h_client;\n",
      "INSERT INTO quests.public_v2.h_episode SELECT * FROM quests.public.h_episode;\n",
      "INSERT INTO quests.public_v2.h_episode_completion SELECT * FROM quests.public.h_episode_completion;\n",
      "INSERT INTO quests.public_v2.h_excursion SELECT * FROM quests.public.h_excursion;\n",
      "INSERT INTO quests.public_v2.h_payment SELECT * FROM quests.public.h_payment;\n",
      "INSERT INTO quests.public_v2.h_promo SELECT * FROM quests.public.h_promo;\n",
      "INSERT INTO quests.public_v2.h_quest SELECT * FROM quests.public.h_quest;\n",
      "INSERT INTO quests.public_v2.h_review SELECT * FROM quests.public.h_review;\n",
      "INSERT INTO quests.public_v2.h_session SELECT * FROM quests.public.h_session;\n",
      "INSERT INTO quests.public_v2.l_author_quest SELECT * FROM quests.public.l_author_quest;\n",
      "INSERT INTO quests.public_v2.l_client_episode_completion SELECT * FROM quests.public.l_client_episode_completion;\n",
      "INSERT INTO quests.public_v2.l_client_review SELECT * FROM quests.public.l_client_review;\n",
      "INSERT INTO quests.public_v2.l_client_session SELECT * FROM quests.public.l_client_session;\n",
      "INSERT INTO quests.public_v2.l_excursion_author SELECT * FROM quests.public.l_excursion_author;\n",
      "INSERT INTO quests.public_v2.l_excursion_category SELECT * FROM quests.public.l_excursion_category;\n",
      "INSERT INTO quests.public_v2.l_excursion_payment SELECT * FROM quests.public.l_excursion_payment;\n",
      "INSERT INTO quests.public_v2.l_excursion_review SELECT * FROM quests.public.l_excursion_review;\n",
      "INSERT INTO quests.public_v2.l_payment_client SELECT * FROM quests.public.l_payment_client;\n",
      "INSERT INTO quests.public_v2.l_payment_promo SELECT * FROM quests.public.l_payment_promo;\n",
      "INSERT INTO quests.public_v2.l_quest_category SELECT * FROM quests.public.l_quest_category;\n",
      "INSERT INTO quests.public_v2.l_quest_episode SELECT * FROM quests.public.l_quest_episode;\n",
      "INSERT INTO quests.public_v2.l_quest_payment SELECT * FROM quests.public.l_quest_payment;\n",
      "INSERT INTO quests.public_v2.l_quest_review SELECT * FROM quests.public.l_quest_review;\n",
      "INSERT INTO quests.public_v2.ref_categories SELECT * FROM quests.public.ref_categories;\n",
      "INSERT INTO quests.public_v2.ref_cities SELECT * FROM quests.public.ref_cities;\n",
      "INSERT INTO quests.public_v2.ref_genres SELECT * FROM quests.public.ref_genres;\n",
      "INSERT INTO quests.public_v2.ref_loyalty_levels SELECT * FROM quests.public.ref_loyalty_levels;\n",
      "INSERT INTO quests.public_v2.ref_professions SELECT * FROM quests.public.ref_professions;\n",
      "INSERT INTO quests.public_v2.ref_sources SELECT * FROM quests.public.ref_sources;\n",
      "INSERT INTO quests.public_v2.s_author_geo_info SELECT * FROM quests.public.s_author_geo_info;\n",
      "INSERT INTO quests.public_v2.s_author_personal_info SELECT * FROM quests.public.s_author_personal_info;\n",
      "INSERT INTO quests.public_v2.s_category_info SELECT * FROM quests.public.s_category_info;\n",
      "INSERT INTO quests.public_v2.s_client_geo_info SELECT * FROM quests.public.s_client_geo_info;\n",
      "INSERT INTO quests.public_v2.s_client_personal_info SELECT * FROM quests.public.s_client_personal_info;\n",
      "INSERT INTO quests.public_v2.s_episode_completion_info SELECT * FROM quests.public.s_episode_completion_info;\n",
      "INSERT INTO quests.public_v2.s_episode_info SELECT * FROM quests.public.s_episode_info;\n",
      "INSERT INTO quests.public_v2.s_excursion_geo_info SELECT * FROM quests.public.s_excursion_geo_info;\n",
      "INSERT INTO quests.public_v2.s_excursion_info SELECT * FROM quests.public.s_excursion_info;\n",
      "INSERT INTO quests.public_v2.s_payment_info SELECT * FROM quests.public.s_payment_info;\n",
      "INSERT INTO quests.public_v2.s_promo_info SELECT * FROM quests.public.s_promo_info;\n",
      "INSERT INTO quests.public_v2.s_quest_info SELECT * FROM quests.public.s_quest_info;\n",
      "INSERT INTO quests.public_v2.s_review_info SELECT * FROM quests.public.s_review_info;\n",
      "INSERT INTO quests.public_v2.s_session_info SELECT * FROM quests.public.s_session_info;\n",
      "ALTER TABLE quests.public_v2.l_excursion_payment ADD COLUMN IF NOT EXISTS sort_order ARRAY(VARCHAR);\n",
      "ALTER TABLE quests.public_v2.l_payment_client ADD COLUMN IF NOT EXISTS sort_order ARRAY(VARCHAR);\n",
      "ALTER TABLE quests.public_v2.l_quest_payment ADD COLUMN IF NOT EXISTS sort_order ARRAY(VARCHAR);\n"
     ]
    }
   ],
   "source": [
    "# create migrations on analize and new_ddl\n",
    "\n",
    "def get_answer(agent, query_summarize, new_ddl):\n",
    "    for _ in range(defolt_retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content=(\n",
    "                    f\"[Context]\\n\"\n",
    "                    f\"Catalog: {CATALOG}\\n\"\n",
    "                    f\"Source schema: {SOURCE_SCHEMA}\\n\"\n",
    "                    f\"Target schema: {TARGET_SCHEMA}\\n\\n\"\n",
    "                    f\"Анализ полученных sql запросов:\\n{query_summarize}\\n\\n\"\n",
    "                    f\"Это Новые DDL запросы:\\n{new_ddl}\"\n",
    "                )\n",
    "            )\n",
    "            resp = agent.invoke({'messages': prompt})[\"messages\"][-1].content\n",
    "            return clean_sql(resp)\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Do retry\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "migrations_creator_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=migrations_creator_prompt(),\n",
    ")\n",
    "\n",
    "migrations = get_answer(migrations_creator_agent, query_summarize, New_DDL)\n",
    "print(migrations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5556475098643b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:44.908555Z",
     "start_time": "2025-10-19T15:48:44.898387Z"
    }
   },
   "outputs": [],
   "source": [
    "codect_info = {\n",
    "    'old_ddl': DDL_querys,\n",
    "    'new_ddl': New_DDL,\n",
    "    'migrations': migrations\n",
    "}\n",
    "\n",
    "judge_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=judge_prompt(),\n",
    ")\n",
    "critic_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=critic_prompt(),\n",
    ")\n",
    "\n",
    "global codect_info, judge_agent, critic_agent\n",
    "\n",
    "async def cheak_query(old_querys, new_querys):\n",
    "    global codect_info, judge_agent, critic_agent\n",
    "\n",
    "    # JUDGE\n",
    "    prompt = HumanMessage(\n",
    "        content=(\n",
    "            f\"[Context]\\n\"\n",
    "            f\"Catalog: {CATALOG}\\n\"\n",
    "            f\"Source schema: {SOURCE_SCHEMA}\\n\"\n",
    "            f\"Target schema: {TARGET_SCHEMA}\\n\\n\"\n",
    "            f\"старые DDL запросы:\\n{codect_info['old_ddl']}\\n\\n\"\n",
    "            f\"Новые DDL запросы:\\n{codect_info['new_ddl']}\\n\\n\"\n",
    "            f\"Вот миграции:\\n{codect_info['migrations']}\\n\\n\"\n",
    "            f\"оригинальный sql запрос:\\n{old_querys}\\n\\n\"\n",
    "            f\"оптимизированный запрос:\\n{new_querys}\\n\"\n",
    "        )\n",
    "    )\n",
    "    judge_response = await judge_agent.ainvoke({'messages': prompt})\n",
    "    cheak_logik = clean_sql(judge_response[\"messages\"][-1].content)\n",
    "\n",
    "    if 'OK' not in cheak_logik:\n",
    "        new_querys = cheak_logik\n",
    "\n",
    "    # CRITIC\n",
    "    prompt = HumanMessage(\n",
    "        content=(\n",
    "            f\"[Context]\\n\"\n",
    "            f\"Catalog: {CATALOG}\\n\"\n",
    "            f\"Source schema: {SOURCE_SCHEMA}\\n\"\n",
    "            f\"Target schema: {TARGET_SCHEMA}\\n\\n\"\n",
    "            f\"старые DDL запросы:\\n{codect_info['old_ddl']}\\n\\n\"\n",
    "            f\"Новые DDL запросы:\\n{codect_info['new_ddl']}\\n\\n\"\n",
    "            f\"Вот миграции:\\n{codect_info['migrations']}\\n\\n\"\n",
    "            f\"оригинальный sql запрос:\\n{old_querys}\\n\\n\"\n",
    "            f\"оптимизированный запрос:\\n{new_querys}\\n\"\n",
    "        )\n",
    "    )\n",
    "    critic_response = await critic_agent.ainvoke({'messages': prompt})\n",
    "    cheak_sintax = clean_sql(critic_response[\"messages\"][-1].content)\n",
    "\n",
    "    if 'OK' not in cheak_sintax:\n",
    "        new_querys = cheak_sintax\n",
    "\n",
    "    return new_querys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410d79505fde58fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:52:25.609959Z",
     "start_time": "2025-10-19T15:48:44.925123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Сборка финального ответа в формате сервиса\n",
    "\n",
    "# Создаём DDL: первая команда — создание схемы, далее только новые таблицы/изменения\n",
    "raw_ddl = [stmt for stmt in split_sql(New_DDL) if stmt.strip()]\n",
    "ddl_entries = []\n",
    "for stmt in raw_ddl:\n",
    "    normalized = rewrite_targets_sources(stmt)\n",
    "    if normalized and not normalized.strip().upper().startswith('SELECT'):\n",
    "        ddl_entries.append({'statement': normalized})\n",
    "\n",
    "# Готовим миграции: только DML для переноса данных в новую схему\n",
    "raw_migrations = [stmt for stmt in split_sql(migrations) if stmt.strip()]\n",
    "migration_entries = []\n",
    "for stmt in raw_migrations:\n",
    "    normalized = rewrite_targets_sources(stmt)\n",
    "    upper = normalized.strip().upper()\n",
    "    if upper.startswith(('INSERT', 'UPDATE', 'MERGE')):\n",
    "        migration_entries.append({'statement': normalized})\n",
    "optim_querys = globals().get('optim_querys', {})\n",
    "# Оптимизированные запросы: обращаются только к новой схеме\n",
    "query_entries = []\n",
    "for item in data['queries']:\n",
    "    qid = item['queryid']\n",
    "    optimized_sql = clean_sql(optim_querys.get(qid, item['query']))\n",
    "    query_entries.append({'queryid': qid, 'query': qualify_query_to_target(optimized_sql)})\n",
    "\n",
    "json_answer = {\n",
    "    'ddl': ddl_entries,\n",
    "    'migrations': migration_entries,\n",
    "    'queries': query_entries,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab72e3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:56:00.996742Z",
     "start_time": "2025-10-19T15:56:00.991917Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('final.json', 'w') as fp:\n",
    "    json.dump(json_answer, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc458905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}